{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from src.benchmark_tft.data_loading import combine_camels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"/Users/cooper/Desktop/CAMELS-CH/data/timeseries/observation_based/\"\n",
    "data_naming_convention = \"CAMELS_CH_obs_based_*.csv\"\n",
    "\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"date\",\n",
    "    \"discharge_spec(mm/d)\",\n",
    "    \"precipitation(mm/d)\",\n",
    "    \"temperature_mean(degC)\",\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_combined = combine_camels_data(\n",
    "    folder_path=data_base_dir,\n",
    "    data_naming_convention=data_naming_convention,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "# Drop rows with NaNs in the discharge column\n",
    "camels_combined = camels_combined.dropna(subset=[\"discharge_spec(mm/d)\"])\n",
    "\n",
    "# Set precipitation and temperature to 0 where NaN\n",
    "camels_combined[\"precipitation(mm/d)\"] = camels_combined[\"precipitation(mm/d)\"].fillna(\n",
    "    0\n",
    ")\n",
    "\n",
    "# Impute temperature with mean\n",
    "camels_combined[\"temperature_mean(degC)\"] = camels_combined[\n",
    "    \"temperature_mean(degC)\"\n",
    "].fillna(camels_combined[\"temperature_mean(degC)\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    camels_combined,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"discharge_spec(mm/d)\",\n",
    "    group_ids=[\"gauge_id\"],\n",
    "    max_encoder_length=30,\n",
    "    max_prediction_length=7,\n",
    "    time_varying_known_reals=[\"precipitation(mm/d)\", \"temperature_mean(degC)\"],\n",
    "    time_varying_unknown_reals=[\"discharge_spec(mm/d)\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"gauge_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "# Create validation set\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    camels_combined,\n",
    "    min_prediction_idx=training.index.time.max() - 30,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=[0] if torch.cuda.is_available() else None,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,\n",
    "    enable_checkpointing=True,\n",
    "    logger=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # Number of quantiles\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
