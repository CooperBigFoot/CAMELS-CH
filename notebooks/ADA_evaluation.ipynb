{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation of ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for all plots\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set_context(\"talk\", font_scale=0.8)\n",
    "\n",
    "\n",
    "def load_experiment_results(results_dir: str, num_runs: int) -> tuple[list, list]:\n",
    "    \"\"\"Load results from multiple experimental runs.\"\"\"\n",
    "    results_dir = Path(results_dir)\n",
    "    challenger_metrics = []\n",
    "    benchmark_metrics = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        try:\n",
    "            # Load challenger results\n",
    "            challenger_file = results_dir / f\"v3/da_challenger_v3_basin_metrics_{run}.csv\"\n",
    "            if challenger_file.exists():\n",
    "                challenger_metrics.append(pd.read_csv(challenger_file))\n",
    "\n",
    "            # Load benchmark results\n",
    "            benchmark_file = results_dir / f\"benchmark_basin_metrics_{run}.csv\"\n",
    "            if benchmark_file.exists():\n",
    "                benchmark_metrics.append(pd.read_csv(benchmark_file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading run {run}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return challenger_metrics, benchmark_metrics\n",
    "\n",
    "\n",
    "def aggregate_metrics(metrics_list: list) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate metrics across multiple runs.\"\"\"\n",
    "    if not metrics_list:\n",
    "        raise ValueError(\"No metrics to aggregate\")\n",
    "\n",
    "    # Stack all runs and group by basin_id and horizon\n",
    "    all_metrics = pd.concat([df.assign(run=i) for i, df in enumerate(metrics_list)])\n",
    "\n",
    "    # Calculate mean and std across runs\n",
    "    agg_metrics = (\n",
    "        all_metrics.groupby([\"basin_id\", \"horizon\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"NSE\": [\"mean\", \"std\"],\n",
    "                \"MSE\": [\"mean\", \"std\"],\n",
    "                \"MAE\": [\"mean\", \"std\"],\n",
    "                \"RMSE\": [\"mean\", \"std\"],\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Flatten column names\n",
    "    agg_metrics.columns = [\n",
    "        \"_\".join(col).strip(\"_\") for col in agg_metrics.columns.values\n",
    "    ]\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "\n",
    "def calculate_performance_comparison(\n",
    "    challenger_df: pd.DataFrame, benchmark_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculate performance comparison between challenger and benchmark models.\"\"\"\n",
    "    # Get unique horizons\n",
    "    horizons = sorted(challenger_df[\"horizon\"].unique())\n",
    "\n",
    "    results = {\n",
    "        \"horizon\": [],\n",
    "        \"better\": [],\n",
    "        \"insignificant\": [],\n",
    "        \"worse\": [],\n",
    "        \"mean_nse_diff\": [],\n",
    "        \"std_nse_diff\": [],\n",
    "    }\n",
    "\n",
    "    for horizon in horizons:\n",
    "        challenger_horizon = challenger_df[challenger_df[\"horizon\"] == horizon]\n",
    "        benchmark_horizon = benchmark_df[benchmark_df[\"horizon\"] == horizon]\n",
    "\n",
    "        comparison = pd.merge(\n",
    "            challenger_horizon,\n",
    "            benchmark_horizon,\n",
    "            on=[\"basin_id\", \"horizon\"],\n",
    "            suffixes=(\"_challenger\", \"_benchmark\"),\n",
    "        )\n",
    "\n",
    "        total_basins = len(comparison)\n",
    "\n",
    "        # Compare mean NSE values\n",
    "        diff = comparison[\"NSE_mean_challenger\"] - comparison[\"NSE_mean_benchmark\"]\n",
    "\n",
    "        # Calculate statistical significance using std across runs\n",
    "        combined_std = np.sqrt(\n",
    "            comparison[\"NSE_std_challenger\"] ** 2 + comparison[\"NSE_std_benchmark\"] ** 2\n",
    "        )\n",
    "        significant_threshold = 1.96 * combined_std  # 95% confidence interval\n",
    "\n",
    "        # Count significant differences\n",
    "        challenger_wins = (diff > significant_threshold).sum()\n",
    "        benchmark_wins = (diff < -significant_threshold).sum()\n",
    "        insignificant = (abs(diff) <= significant_threshold).sum()\n",
    "\n",
    "        results[\"horizon\"].append(horizon)\n",
    "        results[\"better\"].append((challenger_wins / total_basins) * 100)\n",
    "        results[\"insignificant\"].append((insignificant / total_basins) * 100)\n",
    "        results[\"worse\"].append((benchmark_wins / total_basins) * 100)\n",
    "        results[\"mean_nse_diff\"].append(diff.mean())\n",
    "        results[\"std_nse_diff\"].append(diff.std())\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_performance_comparison(comparison_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Create stacked bar plot of performance comparison.\"\"\"\n",
    "    pastel_green = \"#6BA292\"\n",
    "    pastel_red = \"#93827F\"\n",
    "    pastel_yellow = \"#F9E79F\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.bar(\n",
    "        comparison_df[\"horizon\"],\n",
    "        comparison_df[\"better\"],\n",
    "        label=\"Challenger Significantly Better\",\n",
    "        color=pastel_green,\n",
    "    )\n",
    "    plt.bar(\n",
    "        comparison_df[\"horizon\"],\n",
    "        comparison_df[\"insignificant\"],\n",
    "        bottom=comparison_df[\"better\"],\n",
    "        label=\"No Significant Difference\",\n",
    "        color=pastel_yellow,\n",
    "    )\n",
    "    plt.bar(\n",
    "        comparison_df[\"horizon\"],\n",
    "        comparison_df[\"worse\"],\n",
    "        bottom=comparison_df[\"better\"] + comparison_df[\"insignificant\"],\n",
    "        label=\"Benchmark Significantly Better\",\n",
    "        color=pastel_red,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Forecast Horizon (days)\")\n",
    "    plt.ylabel(\"Percentage of Basins\")\n",
    "    plt.title(\"Model Performance Comparison by Horizon\\nWith Statistical Significance\")\n",
    "    # Add the legend below the plot\n",
    "    plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.yticks(np.arange(0, 101, 20), [f\"{i}%\" for i in range(0, 101, 20)])\n",
    "    plt.xticks(comparison_df[\"horizon\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_nse_comparison(\n",
    "    challenger_df: pd.DataFrame, benchmark_df: pd.DataFrame\n",
    ") -> None:\n",
    "    \"\"\"Create bar plot comparing NSE scores between models.\"\"\"\n",
    "    # Calculate mean and confidence intervals for each horizon\n",
    "    horizons = sorted(challenger_df[\"horizon\"].unique())\n",
    "\n",
    "    ch_means = [\n",
    "        challenger_df[challenger_df[\"horizon\"] == h][\"NSE_mean\"].mean()\n",
    "        for h in horizons\n",
    "    ]\n",
    "    ch_stds = [\n",
    "        challenger_df[challenger_df[\"horizon\"] == h][\"NSE_std\"].mean() for h in horizons\n",
    "    ]\n",
    "\n",
    "    bm_means = [\n",
    "        benchmark_df[benchmark_df[\"horizon\"] == h][\"NSE_mean\"].mean() for h in horizons\n",
    "    ]\n",
    "    bm_stds = [\n",
    "        benchmark_df[benchmark_df[\"horizon\"] == h][\"NSE_std\"].mean() for h in horizons\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bar_width = 0.35\n",
    "    x_pos = np.arange(len(horizons))\n",
    "\n",
    "    colors = sns.color_palette(\"Blues\", 2)\n",
    "\n",
    "    # Plot bars with error bars\n",
    "    plt.bar(\n",
    "        x_pos - bar_width / 2,\n",
    "        ch_means,\n",
    "        bar_width,\n",
    "        yerr=ch_stds,\n",
    "        label=\"Challenger\",\n",
    "        color=colors[0],\n",
    "        capsize=5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.bar(\n",
    "        x_pos + bar_width / 2,\n",
    "        bm_means,\n",
    "        bar_width,\n",
    "        yerr=bm_stds,\n",
    "        label=\"Benchmark\",\n",
    "        color=colors[1],\n",
    "        capsize=5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Forecast Horizon (days)\")\n",
    "    plt.ylabel(\"Nash-Sutcliffe Efficiency (NSE)\")\n",
    "    plt.title(\n",
    "        \"Model Performance Comparison by Horizon\\nwith Standard Deviation across Runs\"\n",
    "    )\n",
    "    plt.xticks(x_pos, horizons)\n",
    "    plt.legend()\n",
    "    sns.despine()\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (ch, bm) in enumerate(zip(ch_means, bm_means)):\n",
    "        plt.text(i - bar_width / 2, ch + 0.02, f\"{ch:.2f}\", ha=\"center\")\n",
    "        plt.text(i + bar_width / 2, bm + 0.02, f\"{bm:.2f}\", ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_summary_statistics(comparison_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print summary statistics of the performance comparison.\"\"\"\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(\n",
    "        f\"Average percentage where challenger significantly outperforms: {comparison_df['better'].mean():.1f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average percentage with no significant difference: {comparison_df['insignificant'].mean():.1f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average percentage where benchmark significantly outperforms: {comparison_df['worse'].mean():.1f}%\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\nBest horizon for challenger: {comparison_df.loc[comparison_df['better'].idxmax(), 'horizon']} \"\n",
    "        f\"({comparison_df['better'].max():.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Worst horizon for challenger: {comparison_df.loc[comparison_df['better'].idxmin(), 'horizon']} \"\n",
    "        f\"({comparison_df['better'].min():.1f}%)\"\n",
    "    )\n",
    "\n",
    "    mean_nse_improvement = comparison_df[\"mean_nse_diff\"].mean()\n",
    "    print(f\"\\nMean NSE improvement across all horizons: {mean_nse_improvement:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "results_dir = \"/Users/cooper/Desktop/CAMELS-CH/experiments/AdversarialDomainAdaptation/results\"\n",
    "num_runs = 3  # Update with your number of runs\n",
    "\n",
    "# Load and process results\n",
    "challenger_metrics_list, benchmark_metrics_list = load_experiment_results(\n",
    "    results_dir, num_runs)\n",
    "\n",
    "# Aggregate metrics across runs\n",
    "challenger_agg = aggregate_metrics(challenger_metrics_list)\n",
    "benchmark_agg = aggregate_metrics(benchmark_metrics_list)\n",
    "\n",
    "# Calculate and visualize performance comparison\n",
    "comparison_df = calculate_performance_comparison(challenger_agg, benchmark_agg)\n",
    "plot_performance_comparison(comparison_df)\n",
    "plot_nse_comparison(challenger_agg, benchmark_agg)\n",
    "print_summary_statistics(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_differences(\n",
    "    challenger_df: pd.DataFrame,\n",
    "    benchmark_df: pd.DataFrame,\n",
    "    metric: str = \"NSE\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze the magnitude of performance differences between challenger and benchmark models.\n",
    "\n",
    "    Args:\n",
    "        challenger_df: Aggregated metrics for challenger model\n",
    "        benchmark_df: Aggregated metrics for benchmark model\n",
    "        metric: Metric to analyze (default: NSE)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with analysis results per horizon\n",
    "    \"\"\"\n",
    "    # Get unique horizons\n",
    "    horizons = sorted(challenger_df[\"horizon\"].unique())\n",
    "\n",
    "    results = {\n",
    "        \"horizon\": [],\n",
    "        \"num_better\": [],\n",
    "        \"num_worse\": [],\n",
    "        \"mean_improvement\": [],\n",
    "        \"mean_degradation\": [],\n",
    "        \"max_improvement\": [],\n",
    "        \"max_degradation\": [],\n",
    "        \"median_improvement\": [],\n",
    "        \"median_degradation\": [],\n",
    "    }\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Filter data for current horizon\n",
    "        challenger_horizon = challenger_df[challenger_df[\"horizon\"] == horizon]\n",
    "        benchmark_horizon = benchmark_df[benchmark_df[\"horizon\"] == horizon]\n",
    "\n",
    "        # Merge data\n",
    "        comparison = pd.merge(\n",
    "            challenger_horizon,\n",
    "            benchmark_horizon,\n",
    "            on=[\"basin_id\", \"horizon\"],\n",
    "            suffixes=(\"_challenger\", \"_benchmark\")\n",
    "        )\n",
    "\n",
    "        # Calculate differences and statistical significance\n",
    "        diff = comparison[f\"{metric}_mean_challenger\"] - \\\n",
    "            comparison[f\"{metric}_mean_benchmark\"]\n",
    "        combined_std = np.sqrt(\n",
    "            comparison[f\"{metric}_std_challenger\"]**2 +\n",
    "            comparison[f\"{metric}_std_benchmark\"]**2\n",
    "        )\n",
    "        significant_threshold = 1.96 * combined_std  # 95% confidence interval\n",
    "\n",
    "        # Identify significant improvements and degradations\n",
    "        significant_better = diff > significant_threshold\n",
    "        significant_worse = diff < -significant_threshold\n",
    "\n",
    "        # Calculate statistics for improvements\n",
    "        improvements = diff[significant_better]\n",
    "        degradations = diff[significant_worse]\n",
    "\n",
    "        results[\"horizon\"].append(horizon)\n",
    "        results[\"num_better\"].append(len(improvements))\n",
    "        results[\"num_worse\"].append(len(degradations))\n",
    "        results[\"mean_improvement\"].append(\n",
    "            improvements.mean() if len(improvements) > 0 else np.nan)\n",
    "        results[\"mean_degradation\"].append(\n",
    "            degradations.mean() if len(degradations) > 0 else np.nan)\n",
    "        results[\"max_improvement\"].append(\n",
    "            improvements.max() if len(improvements) > 0 else np.nan)\n",
    "        results[\"max_degradation\"].append(\n",
    "            degradations.min() if len(degradations) > 0 else np.nan)\n",
    "        results[\"median_improvement\"].append(\n",
    "            improvements.median() if len(improvements) > 0 else np.nan)\n",
    "        results[\"median_degradation\"].append(\n",
    "            degradations.median() if len(degradations) > 0 else np.nan)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def print_detailed_analysis(analysis_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print detailed analysis of performance differences.\"\"\"\n",
    "    print(\"\\nDetailed Performance Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Overall statistics\n",
    "    total_better = analysis_df[\"num_better\"].sum()\n",
    "    total_worse = analysis_df[\"num_worse\"].sum()\n",
    "\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"Total cases of significant improvement: {total_better}\")\n",
    "    print(f\"Total cases of significant degradation: {total_worse}\")\n",
    "\n",
    "    if total_better > 0:\n",
    "        print(f\"\\nWhen Challenger Performs Better:\")\n",
    "        print(\n",
    "            f\"Average improvement in NSE: {analysis_df['mean_improvement'].mean():.3f}\")\n",
    "        print(\n",
    "            f\"Maximum improvement in NSE: {analysis_df['max_improvement'].max():.3f}\")\n",
    "        print(\n",
    "            f\"Median improvement in NSE: {analysis_df['median_improvement'].mean():.3f}\")\n",
    "\n",
    "    if total_worse > 0:\n",
    "        print(f\"\\nWhen Challenger Performs Worse:\")\n",
    "        print(\n",
    "            f\"Average degradation in NSE: {analysis_df['mean_degradation'].mean():.3f}\")\n",
    "        print(\n",
    "            f\"Maximum degradation in NSE: {analysis_df['mean_degradation'].min():.3f}\")\n",
    "        print(\n",
    "            f\"Median degradation in NSE: {analysis_df['median_degradation'].mean():.3f}\")\n",
    "\n",
    "    print(\"\\nBreakdown by Horizon:\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in analysis_df.iterrows():\n",
    "        print(f\"\\nHorizon {int(row['horizon'])}:\")\n",
    "        if row['num_better'] > 0:\n",
    "            print(f\"  Improvements ({row['num_better']} basins):\")\n",
    "            print(f\"    Mean: {row['mean_improvement']:.3f}\")\n",
    "            print(f\"    Median: {row['median_improvement']:.3f}\")\n",
    "            print(f\"    Max: {row['max_improvement']:.3f}\")\n",
    "        if row['num_worse'] > 0:\n",
    "            print(f\"  Degradations ({row['num_worse']} basins):\")\n",
    "            print(f\"    Mean: {row['mean_degradation']:.3f}\")\n",
    "            print(f\"    Median: {row['median_degradation']:.3f}\")\n",
    "            print(f\"    Max: {row['max_degradation']:.3f}\")\n",
    "\n",
    "\n",
    "def plot_performance_differences(analysis_df: pd.DataFrame) -> None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    x = analysis_df['horizon'].values\n",
    "    width = 0.35\n",
    "\n",
    "    # Plot improvements and degradations\n",
    "    plt.bar(x - width/2, analysis_df['median_improvement'],\n",
    "            width, label='Significant Improvements', color='#6BA292')\n",
    "    plt.bar(x + width/2, analysis_df['median_degradation'],\n",
    "            width, label='Significant Degradations', color='#93827F')\n",
    "\n",
    "    # Add reference line at y=0\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Customize plot\n",
    "    plt.xlabel('Forecast Horizon (days)')\n",
    "    plt.ylabel('Median NSE Difference\\n(Challenger - Benchmark)')\n",
    "    plt.title('Median Performance Differences by Horizon')\n",
    "    plt.legend()\n",
    "\n",
    "    # Set x-axis ticks\n",
    "    plt.xticks(x)\n",
    "\n",
    "    plt.ylim(-0.06, 0.06)\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Remove top and right spines\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance differences\n",
    "analysis_df = analyze_performance_differences(challenger_agg, benchmark_agg)\n",
    "print_detailed_analysis(analysis_df)\n",
    "plot_performance_differences(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_basin_performance(\n",
    "    challenger_df: pd.DataFrame,\n",
    "    benchmark_df: pd.DataFrame,\n",
    "    metric: str = \"NSE\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Identify basins where challenger significantly improves or degrades performance.\n",
    "    \n",
    "    Args:\n",
    "        challenger_df: Aggregated metrics for challenger model\n",
    "        benchmark_df: Aggregated metrics for benchmark model\n",
    "        metric: Metric to analyze (default: NSE)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing improved and degraded basins by horizon\n",
    "    \"\"\"\n",
    "    # Get unique horizons\n",
    "    horizons = sorted(challenger_df[\"horizon\"].unique())\n",
    "    \n",
    "    results = {\n",
    "        'improved': {h: [] for h in horizons},\n",
    "        'degraded': {h: [] for h in horizons},\n",
    "        'summary': {\n",
    "            'consistently_improved': set(),\n",
    "            'consistently_degraded': set(),\n",
    "            'mixed_performance': set()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Track performance across all horizons\n",
    "    all_improved_basins = set()\n",
    "    all_degraded_basins = set()\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        # Filter data for current horizon\n",
    "        challenger_horizon = challenger_df[challenger_df[\"horizon\"] == horizon]\n",
    "        benchmark_horizon = benchmark_df[benchmark_df[\"horizon\"] == horizon]\n",
    "        \n",
    "        # Merge data\n",
    "        comparison = pd.merge(\n",
    "            challenger_horizon,\n",
    "            benchmark_horizon,\n",
    "            on=[\"basin_id\", \"horizon\"],\n",
    "            suffixes=(\"_challenger\", \"_benchmark\")\n",
    "        )\n",
    "        \n",
    "        # Calculate differences and statistical significance\n",
    "        diff = comparison[f\"{metric}_mean_challenger\"] - comparison[f\"{metric}_mean_benchmark\"]\n",
    "        combined_std = np.sqrt(\n",
    "            comparison[f\"{metric}_std_challenger\"]**2 + \n",
    "            comparison[f\"{metric}_std_benchmark\"]**2\n",
    "        )\n",
    "        significant_threshold = 1.96 * combined_std  # 95% confidence interval\n",
    "        \n",
    "        # Identify basins with significant differences\n",
    "        improved_mask = diff > significant_threshold\n",
    "        degraded_mask = diff < -significant_threshold\n",
    "        \n",
    "        # Store basin IDs\n",
    "        improved_basins = comparison.loc[improved_mask, \"basin_id\"].tolist()\n",
    "        degraded_basins = comparison.loc[degraded_mask, \"basin_id\"].tolist()\n",
    "        \n",
    "        results['improved'][horizon] = improved_basins\n",
    "        results['degraded'][horizon] = degraded_basins\n",
    "        \n",
    "        # Update sets of all improved/degraded basins\n",
    "        all_improved_basins.update(improved_basins)\n",
    "        all_degraded_basins.update(degraded_basins)\n",
    "    \n",
    "    # Identify basins that appear in both sets\n",
    "    mixed_basins = all_improved_basins.intersection(all_degraded_basins)\n",
    "    \n",
    "    # Update summary\n",
    "    results['summary']['consistently_improved'] = all_improved_basins - mixed_basins\n",
    "    results['summary']['consistently_degraded'] = all_degraded_basins - mixed_basins\n",
    "    results['summary']['mixed_performance'] = mixed_basins\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_basin_analysis(results: dict) -> None:\n",
    "    \"\"\"Print analysis of basin performance.\"\"\"\n",
    "    print(\"\\nBasin Performance Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nConsistently Improved Basins:\")\n",
    "    print(f\"Total: {len(results['summary']['consistently_improved'])}\")\n",
    "    print(sorted(results['summary']['consistently_improved']))\n",
    "    \n",
    "    print(\"\\nConsistently Degraded Basins:\")\n",
    "    print(f\"Total: {len(results['summary']['consistently_degraded'])}\")\n",
    "    print(sorted(results['summary']['consistently_degraded']))\n",
    "    \n",
    "    print(\"\\nMixed Performance Basins:\")\n",
    "    print(f\"Total: {len(results['summary']['mixed_performance'])}\")\n",
    "    print(sorted(results['summary']['mixed_performance']))\n",
    "    \n",
    "    print(\"\\nBreakdown by Horizon:\")\n",
    "    print(\"-\" * 80)\n",
    "    for horizon in sorted(results['improved'].keys()):\n",
    "        improved = results['improved'][horizon]\n",
    "        degraded = results['degraded'][horizon]\n",
    "        \n",
    "        print(f\"\\nHorizon {horizon}:\")\n",
    "        print(f\"  Improved ({len(improved)} basins):\")\n",
    "        print(f\"    {sorted(improved)}\")\n",
    "        print(f\"  Degraded ({len(degraded)} basins):\")\n",
    "        print(f\"    {sorted(degraded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify basins with significant improvements/degradations\n",
    "basin_results = identify_basin_performance(challenger_agg, benchmark_agg)\n",
    "print_basin_analysis(basin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load and process the data\n",
    "path_to_kgz_basins = \"/Users/cooper/Desktop/CAMELS-CH/data/CA_raw/basin_outline/kyrgyzstan/HRU_KRG_ML_MODEL_BASINS_1d.shp\"\n",
    "path_to_tjik_basins = \"/Users/cooper/Desktop/CAMELS-CH/data/CA_raw/basin_outline/tajikistan/HRU_TAJIK_ML_MODEL_BASINS_1d.shp\"\n",
    "\n",
    "kgz_basins = gpd.read_file(path_to_kgz_basins).to_crs(epsg=4326)\n",
    "tjik_basins = gpd.read_file(path_to_tjik_basins).to_crs(epsg=4326)\n",
    "\n",
    "# Define the basins of interest\n",
    "improved_basins = ['CA_15045', 'CA_15083',\n",
    "                   'CA_15259', 'CA_16136', 'CA_17050', 'CA_17100']\n",
    "degraded_basins = ['CA_15039', 'CA_16055', 'CA_16068', 'CA_17137']\n",
    "\n",
    "# Create masks for different basin categories\n",
    "kgz_improved = kgz_basins['CODE'].astype(\n",
    "    str).apply(lambda x: f'CA_{x}' in improved_basins)\n",
    "kgz_degraded = kgz_basins['CODE'].astype(\n",
    "    str).apply(lambda x: f'CA_{x}' in degraded_basins)\n",
    "tjik_improved = tjik_basins['CODE'].astype(\n",
    "    str).apply(lambda x: f'CA_{x}' in improved_basins)\n",
    "tjik_degraded = tjik_basins['CODE'].astype(\n",
    "    str).apply(lambda x: f'CA_{x}' in degraded_basins)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot regular basins\n",
    "kgz_basins[~(kgz_improved | kgz_degraded)].plot(ax=ax, color='lightgrey',\n",
    "                                                edgecolor='black', alpha=0.5)\n",
    "tjik_basins[~(tjik_improved | tjik_degraded)].plot(\n",
    "    ax=ax, color='lightgrey', edgecolor='black', alpha=0.5)\n",
    "\n",
    "# Plot improved basins\n",
    "kgz_basins[kgz_improved].plot(ax=ax, color='#6BA292', edgecolor='black')\n",
    "tjik_basins[tjik_improved].plot(ax=ax, color='#6BA292', edgecolor='black')\n",
    "\n",
    "# Plot degraded basins\n",
    "kgz_basins[kgz_degraded].plot(ax=ax, color='#93827F', edgecolor='black')\n",
    "tjik_basins[tjik_degraded].plot(ax=ax, color='#93827F', edgecolor='black')\n",
    "\n",
    "# Manually add legend\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color='lightgrey', label='Other Basins'),\n",
    "    mpatches.Patch(color='#6BA292', label='Improved Basins'),\n",
    "    mpatches.Patch(color='#93827F', label='Degraded Basins')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_patches, loc=\"lower right\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add gridlines\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Remove axis labels as they're not needed for a map\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "\n",
    "# Adjust layout to prevent legend cutoff\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
