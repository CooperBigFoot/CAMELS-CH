{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids\n",
    "from src.data_models.dataset import HydroDataset\n",
    "from src.data_models.preprocessing import (\n",
    "    scale_time_series,\n",
    "    scale_static_attributes,\n",
    "    inverse_scale_static_attributes,\n",
    "    inverse_scale_time_series,\n",
    ")\n",
    "from src.data_models.caravanify import Caravanify, CaravanifyConfig\n",
    "\n",
    "from utils.metrics import nash_sutcliffe_efficiency\n",
    "from src.data_models.datamodule import HydroDataModule\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.preprocessing.transformers import GroupedTransformer, LogTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import LitLSTM\n",
    "from src.models.ealstm import LitEALSTM\n",
    "from src.models.TSMixer import LitTSMixer\n",
    "from src.models.evaluators import TSForecastEvaluator\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/timeseries/csv\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "\n",
    "caravan = Caravanify(config)\n",
    "ids_for_training = caravan.get_all_gauge_ids()[:3]\n",
    "\n",
    "print(f\"Total number of stations: {len(ids_for_training)}\")\n",
    "\n",
    "caravan.load_stations(ids_for_training)\n",
    "\n",
    "ts_data = caravan.get_time_series()\n",
    "static_data = caravan.get_static_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data[\"date\"] = pd.to_datetime(ts_data[\"date\"])\n",
    "\n",
    "ts_data[\"julian_day\"] = ts_data[\"date\"].dt.dayofyear\n",
    "\n",
    "ts_columns = [\n",
    "    # \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    # \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"streamflow\",\n",
    "    # \"julian_day\",\n",
    "    # \"temperature_2m_mean\",\n",
    "    \"total_precipitation_sum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = ts_columns + [\"gauge_id\", \"date\"]\n",
    "ts_data = ts_data[whole_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statics_to_keep = [\n",
    "    \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "static_columns = static_data.columns\n",
    "static_columns = [col for col in list(static_columns) if col in statics_to_keep]\n",
    "\n",
    "static_data = static_data[static_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    col for col in ts_data.columns if col not in [\"gauge_id\", \"date\", \"streamflow\"]\n",
    "]\n",
    "ts_columns = features + [\"streamflow\"]  # Ensure target is not in features\n",
    "\n",
    "# Feature pipeline: log + scale\n",
    "feature_pipeline = Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "target_pipeline = GroupedTransformer(\n",
    "    Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "preprocessing_configs = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 10\n",
    "static_columns = [c for c in static_columns if c not in [\"gauge_id\"]]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
    "    input_length = trial.suggest_int(\"input_length\", 14, 60)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)\n",
    "\n",
    "    # Create data module with the trial's batch size and input length\n",
    "    data_module = HydroDataModule(\n",
    "        time_series_df=ts_data,\n",
    "        static_df=static_data,\n",
    "        group_identifier=\"gauge_id\",\n",
    "        preprocessing_config=preprocessing_configs,\n",
    "        batch_size=batch_size,  # Use trial's batch size\n",
    "        input_length=input_length,  # Use trial's input length\n",
    "        output_length=output_length,\n",
    "        num_workers=4,\n",
    "        features=ts_columns,\n",
    "        static_features=static_columns,\n",
    "        target=\"streamflow\",\n",
    "        min_train_years=2,\n",
    "        val_years=1,\n",
    "        test_years=3,\n",
    "        max_missing_pct=10,\n",
    "    )\n",
    "\n",
    "    # Create model with trial's hidden size\n",
    "    model = LitTSMixer(\n",
    "        input_len=input_length,  # Match data module's input length\n",
    "        output_len=output_length,\n",
    "        input_size=len(ts_columns),\n",
    "        static_size=len(static_columns),\n",
    "        hidden_size=hidden_size,  # Use trial's hidden size\n",
    "    )\n",
    "\n",
    "    # Configure trainer with early stopping\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,  # Keep this low for initial testing\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")],\n",
    "        enable_progress_bar=False,  # Reduce output clutter during optimization\n",
    "    )\n",
    "\n",
    "    # Train and get the best validation loss\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "# Create a study object and specify the direction of optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=10)  # Start with 10 trials for testing\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)\n",
    "\n",
    "# You can also print a summary of the optimization\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\n",
    "    \"  Number of pruned trials: \",\n",
    "    len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),\n",
    ")\n",
    "print(\n",
    "    \"  Number of complete trials: \",\n",
    "    len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
