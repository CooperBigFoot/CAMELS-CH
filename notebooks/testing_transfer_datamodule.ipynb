{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids\n",
    "from src.data_models.dataset import HydroDataset\n",
    "from src.data_models.preprocessing import (\n",
    "    scale_time_series,\n",
    "    scale_static_attributes,\n",
    "    inverse_scale_static_attributes,\n",
    "    inverse_scale_time_series,\n",
    ")\n",
    "from src.data_models.caravanify import Caravanify, CaravanifyConfig\n",
    "\n",
    "from utils.metrics import nash_sutcliffe_efficiency\n",
    "from src.data_models.datamodule import HydroTransferDataModule, HydroDataModule\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.preprocessing.transformers import GroupedTransformer, LogTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import LitLSTM\n",
    "from src.models.ealstm import LitEALSTM\n",
    "from src.models.TSMixer import LitTSMixer, TSMixerConfig\n",
    "from src.models.TSMixerDomainAdaptation import LitTSMixerDomainAdaptation\n",
    "from src.models.evaluators import TSForecastEvaluator\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stations: 3\n"
     ]
    }
   ],
   "source": [
    "# Configuration for loading Central Asian (CA) hydrology data\n",
    "CA_config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/timeseries/csv\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "# Initialize Caravan data loader and load first 3 stations for training\n",
    "CA_caravan = Caravanify(CA_config)\n",
    "ids_for_training = CA_caravan.get_all_gauge_ids()[:3]\n",
    "print(f\"Total number of stations: {len(ids_for_training)}\")\n",
    "CA_caravan.load_stations(ids_for_training)\n",
    "\n",
    "# Get time series and static data\n",
    "CA_ts_data = CA_caravan.get_time_series()\n",
    "CA_static_data = CA_caravan.get_static_attributes()\n",
    "\n",
    "# Process time series data\n",
    "CA_ts_data[\"date\"] = pd.to_datetime(CA_ts_data[\"date\"])\n",
    "CA_ts_data[\"julian_day\"] = CA_ts_data[\"date\"].dt.dayofyear\n",
    "\n",
    "# Select relevant time series features\n",
    "ts_columns = [\"streamflow\", \"total_precipitation_sum\"]\n",
    "CA_ts_data = CA_ts_data[ts_columns + [\"gauge_id\", \"date\"]]\n",
    "\n",
    "# Select relevant static features that characterize catchment properties\n",
    "static_columns = [\n",
    "    \"gauge_id\", \"p_mean\", \"area\", \"ele_mt_sav\", \"high_prec_dur\",\n",
    "    \"frac_snow\", \"high_prec_freq\", \"slp_dg_sav\", \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\", \"aridity_FAO_PM\"\n",
    "]\n",
    "CA_static_data = CA_static_data[static_columns]\n",
    "\n",
    "# Separate features from target variable\n",
    "features = [col for col in CA_ts_data.columns if col not in [\"gauge_id\", \"date\", \"streamflow\"]]\n",
    "ts_columns = features + [\"streamflow\"]\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    (\"log\", LogTransformer()), \n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "target_pipeline = GroupedTransformer(\n",
    "    Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "preprocessing_configs = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original basins: 3\n",
      "Retained basins: 2\n",
      "Domain target: Created 13284 valid sequences from 2 catchments\n",
      "Domain target: Created 652 valid sequences from 2 catchments\n",
      "Domain target: Created 2114 valid sequences from 2 catchments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/src/data_models/datamodule.py:354: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.processed_static[col] = transformed[:, i]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_length = 10\n",
    "input_length = 30\n",
    "hidden_size = 15\n",
    "\n",
    "# Create data module with the trial's batch size and input length\n",
    "CA_data_module = HydroDataModule(\n",
    "    time_series_df=CA_ts_data,\n",
    "    static_df=CA_static_data,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    preprocessing_config=preprocessing_configs,\n",
    "    batch_size=batch_size,  # Use trial's batch size\n",
    "    input_length=input_length,  # Use trial's input length\n",
    "    output_length=output_length,\n",
    "    num_workers=4,\n",
    "    features=ts_columns,\n",
    "    static_features=static_columns,\n",
    "    target=\"streamflow\",\n",
    "    min_train_years=2,\n",
    "    val_years=1,\n",
    "    test_years=3,\n",
    "    max_missing_pct=10,\n",
    "    domain_id=\"target\",\n",
    ")\n",
    "\n",
    "CA_data_module.prepare_data()\n",
    "CA_data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switzerland "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stations: 3\n"
     ]
    }
   ],
   "source": [
    "# Configuration for loading Central Asian (CA) hydrology data\n",
    "CH_config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CH/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CH/post_processed/timeseries/csv\",\n",
    "    gauge_id_prefix=\"CH\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "# Initialize Caravan data loader and load first 3 stations for training\n",
    "CH_caravan = Caravanify(CH_config)\n",
    "ids_for_training = CH_caravan.get_all_gauge_ids()[:3]\n",
    "print(f\"Total number of stations: {len(ids_for_training)}\")\n",
    "CH_caravan.load_stations(ids_for_training)\n",
    "\n",
    "# Get time series and static data\n",
    "CH_ts_data = CH_caravan.get_time_series()\n",
    "CH_static_data = CH_caravan.get_static_attributes()\n",
    "\n",
    "# Process time series data\n",
    "CH_ts_data[\"date\"] = pd.to_datetime(CH_ts_data[\"date\"])\n",
    "CH_ts_data[\"julian_day\"] = CH_ts_data[\"date\"].dt.dayofyear\n",
    "\n",
    "# Select relevant time series features\n",
    "ts_columns = [\"streamflow\", \"total_precipitation_sum\"]\n",
    "CH_ts_data = CH_ts_data[ts_columns + [\"gauge_id\", \"date\"]]\n",
    "\n",
    "# Select relevant static features that characterize catchment properties\n",
    "static_columns = [\n",
    "    \"gauge_id\", \"p_mean\", \"area\", \"ele_mt_sav\", \"high_prec_dur\",\n",
    "    \"frac_snow\", \"high_prec_freq\", \"slp_dg_sav\", \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\", \"aridity_FAO_PM\"\n",
    "]\n",
    "CH_static_data = CH_static_data[static_columns]\n",
    "\n",
    "# Separate features from target variable\n",
    "features = [col for col in CH_ts_data.columns if col not in [\n",
    "    \"gauge_id\", \"date\", \"streamflow\"]]\n",
    "ts_columns = features + [\"streamflow\"]\n",
    "\n",
    "# Define preprocessing pipelines:\n",
    "# 1. Feature pipeline: Log transform followed by standardization\n",
    "feature_pipeline = Pipeline([\n",
    "    (\"log\", LogTransformer()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Target pipeline: Log transform and standardization per catchment\n",
    "target_pipeline = GroupedTransformer(\n",
    "    Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "# 3. Static feature pipeline: Only standardization needed\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "# Combine all preprocessing configurations\n",
    "preprocessing_configs = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original basins: 3\n",
      "Retained basins: 3\n",
      "Domain source: Created 39327 valid sequences from 3 catchments\n",
      "Domain source: Created 978 valid sequences from 3 catchments\n",
      "Domain source: Created 3171 valid sequences from 3 catchments\n"
     ]
    }
   ],
   "source": [
    "CH_data_module = HydroDataModule(\n",
    "    time_series_df=CH_ts_data,\n",
    "    static_df=CH_static_data,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    preprocessing_config=preprocessing_configs,\n",
    "    batch_size=batch_size,  # Use trial's batch size\n",
    "    input_length=input_length,  # Use trial's input length\n",
    "    output_length=output_length,\n",
    "    num_workers=4,\n",
    "    features=ts_columns,\n",
    "    static_features=static_columns,\n",
    "    target=\"streamflow\",\n",
    "    min_train_years=2,\n",
    "    val_years=1,\n",
    "    test_years=3,\n",
    "    max_missing_pct=10,\n",
    "    domain_id=\"source\",\n",
    ")\n",
    "\n",
    "CH_data_module.prepare_data()\n",
    "CH_data_module.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing combined DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transfer datamodule\n",
    "transfer_dm = HydroTransferDataModule(\n",
    "    source_datamodule=CA_data_module,\n",
    "    target_datamodule=CH_data_module,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Setup and get training loader\n",
    "transfer_dm.setup(\"fit\")\n",
    "train_loader = transfer_dm.train_dataloader()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 6:  # Look at first 6 batches\n",
    "        break\n",
    "    print(f\"\\nMixed Batch {i}:\")\n",
    "    print(f\"X shape: {batch['X'].shape}\")\n",
    "    print(f\"y shape: {batch['y'].shape}\")\n",
    "    print(f\"static shape: {batch['static'].shape}\")\n",
    "    print(f\"domain_id shape: {batch['domain_id'].shape}\")\n",
    "\n",
    "    # Count samples from each domain using float tensor comparison\n",
    "    source_samples = (batch['domain_id'] == 0.0).sum().item()\n",
    "    target_samples = (batch['domain_id'] == 1.0).sum().item()\n",
    "\n",
    "    print(f\"Distribution:\")\n",
    "    print(f\"- Source (CA): {source_samples} samples\")\n",
    "    print(f\"- Target (CH): {target_samples} samples\")\n",
    "    print(f\"- Total: {source_samples + target_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Initialize the model with domain adaptation\n",
    "model = LitTSMixerDomainAdaptation(\n",
    "    config=TSMixerConfig(\n",
    "        input_len=input_length,\n",
    "        output_len=output_length,\n",
    "        input_size=2,\n",
    "        static_size=10,\n",
    "        hidden_size=hidden_size\n",
    "    ),\n",
    "    lambda_adv=1.0,\n",
    "    domain_loss_weight=0.1,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3)],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, transfer_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, CA_data_module)\n",
    "raw_results = model.test_results\n",
    "\n",
    "# Create evaluator and get metrics\n",
    "evaluator = TSForecastEvaluator(\n",
    "    CA_data_module, horizons=list(range(1, model.config.output_len + 1))\n",
    ")\n",
    "results_df, overall_metrics, basin_metrics = evaluator.evaluate(raw_results)\n",
    "\n",
    "# Get overall summary\n",
    "overall_summary = evaluator.summarize_metrics(overall_metrics)\n",
    "\n",
    "# Get per-basin summary\n",
    "basin_summary = evaluator.summarize_metrics(basin_metrics, per_basin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_metric_summary(\n",
    "    summary_df: pd.DataFrame, metric: str, per_basin: bool = False, figsize=(10, 6)\n",
    "):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if per_basin:\n",
    "        df_plot = summary_df[metric].unstack(level=0)\n",
    "\n",
    "        # Sort basins based on first horizon values\n",
    "        first_horizon_values = df_plot.iloc[0]\n",
    "        sorted_basins = first_horizon_values.sort_values(ascending=False).index\n",
    "        df_plot = df_plot[sorted_basins]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=df_plot.melt(ignore_index=False).reset_index(),\n",
    "            x=\"horizon\",\n",
    "            y=\"value\",\n",
    "            hue=\"basin_id\",\n",
    "            palette=\"Blues\",\n",
    "        )\n",
    "        plt.title(f\"{metric} by Basin and Horizon\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", title=\"Basin ID\")\n",
    "\n",
    "    else:\n",
    "        ax = sns.barplot(x=summary_df.index, y=summary_df[metric], color=\"steelblue\")\n",
    "        plt.title(f\"Overall {metric} by Horizon\")\n",
    "\n",
    "        for i, v in enumerate(summary_df[metric]):\n",
    "            ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xlabel(\"Forecast Horizon\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "plot_metric_summary(overall_summary, \"NSE\")  # Plot overall NSE\n",
    "plot_metric_summary(\n",
    "    basin_summary, \"NSE\", per_basin=True, figsize=(12, 6)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
