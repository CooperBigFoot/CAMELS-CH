{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids\n",
    "from src.data_models.dataset import HydroDataset\n",
    "from src.data_models.preprocessing import (\n",
    "    scale_time_series,\n",
    "    scale_static_attributes,\n",
    "    inverse_scale_static_attributes,\n",
    "    inverse_scale_time_series,\n",
    ")\n",
    "from src.data_models.caravanify import Caravanify, CaravanifyConfig\n",
    "\n",
    "from utils.metrics import nash_sutcliffe_efficiency\n",
    "from src.data_models.datamodule import HydroDataModule\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.preprocessing.transformers import GroupedTransformer, LogTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Caravanify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/timeseries/csv\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "\n",
    "caravan = Caravanify(config)\n",
    "# ids_for_training = [\n",
    "#     \"CA_15016\",\n",
    "#     \"CA_17462\",\n",
    "# ]\n",
    "ids_for_training = caravan.get_all_gauge_ids()[:3]\n",
    "\n",
    "print(f\"Total number of stations: {len(ids_for_training)}\")\n",
    "\n",
    "caravan.load_stations(ids_for_training)\n",
    "\n",
    "\n",
    "# Get data\n",
    "ts_data = caravan.get_time_series()  \n",
    "static_data = caravan.get_static_attributes()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data[\"date\"] = pd.to_datetime(ts_data[\"date\"])\n",
    "\n",
    "# Now we can get the day of year using dt accessor\n",
    "ts_data[\"julian_day\"] = ts_data[\"date\"].dt.dayofyear\n",
    "\n",
    "# Get column names excluding specific columns\n",
    "ts_columns = [\n",
    "    # \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    # \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"streamflow\",\n",
    "    # \"julian_day\",\n",
    "    # \"temperature_2m_mean\",\n",
    "    \"total_precipitation_sum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = ts_columns + [\"gauge_id\", \"date\"]\n",
    "ts_data = ts_data[whole_data]\n",
    "\n",
    "# # Group by gauge_id \n",
    "# grouped = ts_data.groupby(\"gauge_id\")\n",
    "\n",
    "# # Plot time series for each gauge of the ts_columns of the last 5 years\n",
    "# for gauge_id, group in grouped:\n",
    "#     group = group.set_index(\"date\")\n",
    "#     group = group.loc[\"2015-01-01\":\"2020-12-31\"]\n",
    "#     group[ts_columns].plot(subplots=True, figsize=(20, 20), title=gauge_id)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statics_to_keep = [\n",
    "    \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "static_columns = static_data.columns\n",
    "static_columns = [col for col in list(static_columns) if col in statics_to_keep]\n",
    "\n",
    "static_data = static_data[static_columns]\n",
    "static_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare CAMELS-CH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camels_config = CamelsCHConfig(\n",
    "#     timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/timeseries/observation_based/\",\n",
    "#     timeseries_pattern=\"CAMELS_CH_obs_based_*.csv\",\n",
    "#     static_attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/static_attributes\",\n",
    "#     use_climate=False,\n",
    "#     use_geology=False,\n",
    "#     use_glacier=False,\n",
    "#     use_human_influence=False,\n",
    "#     use_hydrogeology=False,\n",
    "#     use_hydrology=False,\n",
    "#     use_landcover=False,\n",
    "#     use_soil=False,\n",
    "#     use_topographic=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_gauge_ids = get_all_gauge_ids(camels_config)\n",
    "\n",
    "# ids_for_training = all_gauge_ids[:5]\n",
    "\n",
    "# camels = CamelsCH(camels_config)\n",
    "# camels.load_stations(ids_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = camels.get_time_series()\n",
    "# data = data[\n",
    "#     [\n",
    "#         \"gauge_id\",\n",
    "#         \"date\",\n",
    "#         \"discharge_spec(mm/d)\",\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static = camels.get_static_attributes()\n",
    "# sc = static.columns\n",
    "\n",
    "# # for i in range(len(sc)):\n",
    "# #     print(f\"{i}: {sc[i]}\")\n",
    "# static_attributes = [\n",
    "#     \"gauge_id\",\n",
    "#     \"area\", \n",
    "#     \"elev_mean\",  \n",
    "#     \"slope_mean\",  \n",
    "#     \"aridity\",  \n",
    "#     \"p_seasonality\",  \n",
    "#     \"frac_snow\",  \n",
    "#     \"porosity\",  \n",
    "#     \"conductivity\",  \n",
    "#     \"p_mean\",  \n",
    "#     \"geo_porosity\",  \n",
    "# ]\n",
    "# static = static[static_attributes]\n",
    "# static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_config = {\n",
    "#     \"features\": {\n",
    "#         \"scale_method\": \"per_basin\",\n",
    "#         \"log_transform\": []\n",
    "#     },\n",
    "#     \"target\": {\n",
    "#         \"scale_method\": \"per_basin\",\n",
    "#         \"log_transform\": False\n",
    "#     },\n",
    "#     \"static_features\": {\n",
    "#         \"scale_method\": \"global\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "features = [\n",
    "    col for col in ts_data.columns if col not in [\"gauge_id\", \"date\", \"streamflow\"]\n",
    "]\n",
    "ts_columns = features + [\"streamflow\"]  # Ensure target is not in features\n",
    "\n",
    "# Feature pipeline: log + scale\n",
    "feature_pipeline = Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "target_pipeline = GroupedTransformer(\n",
    "    Pipeline([(\"log\", LogTransformer()), (\"scaler\", StandardScaler())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "preprocessing_configs = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 10\n",
    "\n",
    "static_columns = [c for c in static_columns if c not in [\"gauge_id\"]]\n",
    "\n",
    "print(\"TS columns:\", ts_columns)\n",
    "print(\"Static columns:\", static_columns)\n",
    "\n",
    "\n",
    "\n",
    "data_module = HydroDataModule(\n",
    "    time_series_df=ts_data,\n",
    "    static_df=static_data,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    preprocessing_config=preprocessing_configs,\n",
    "    batch_size=32,\n",
    "    input_length=30,\n",
    "    output_length=output_length,\n",
    "    num_workers=4,\n",
    "    features=ts_columns,\n",
    "    static_features=static_columns,\n",
    "    target=\"streamflow\",\n",
    "    min_train_years=2,\n",
    "    val_years=1,\n",
    "    test_years=3,\n",
    "    max_missing_pct=10,\n",
    "    domain_id=\"CA\",\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "# # Now check the processing\n",
    "# print(\"Static features:\", data_module.static_features)\n",
    "# print(\"Static data processed:\", data_module.processed_static is not None)\n",
    "# print(\"Static scalers present:\", \"static\" in data_module.scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import LitLSTM\n",
    "from src.models.ealstm import LitEALSTM\n",
    "from src.models.TSMixer import LitTSMixer\n",
    "from src.models.evaluators import TSForecastEvaluator\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evalue and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LitLSTM(\n",
    "#     input_size=len(ts_columns),\n",
    "#     hidden_size=16,\n",
    "#     num_layers=1,\n",
    "#     output_size=output_length,\n",
    "#     target=data_module.target,\n",
    "# )\n",
    "\n",
    "# model = LitEALSTM(\n",
    "#     input_size_dyn=len(ts_columns),\n",
    "#     input_size_stat=len(static_columns) - 1,\n",
    "#     hidden_size=64,\n",
    "#     output_size=output_length,\n",
    "# )\n",
    "\n",
    "model = LitTSMixer(\n",
    "    input_len=30,\n",
    "    output_len=output_length,\n",
    "    input_size=len(ts_columns),\n",
    "    static_size=len(static_columns),\n",
    "    hidden_size=64,\n",
    ")\n",
    "\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = data_module.quality_report\n",
    "\n",
    "excluded_basins = list(quality_report[\"excluded_basins\"].keys())\n",
    "excluded_basins\n",
    "\n",
    "ids_for_training = [id for id in ids_for_training if id not in excluded_basins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)\n",
    "raw_results = model.test_results\n",
    "\n",
    "# Create evaluator and get metrics\n",
    "evaluator = TSForecastEvaluator(\n",
    "    data_module, horizons=list(range(1, model.config.pred_len + 1))\n",
    ")\n",
    "results_df, overall_metrics, basin_metrics = evaluator.evaluate(raw_results)\n",
    "\n",
    "# Get overall summary\n",
    "overall_summary = evaluator.summarize_metrics(overall_metrics)\n",
    "\n",
    "# Get per-basin summary\n",
    "basin_summary = evaluator.summarize_metrics(basin_metrics, per_basin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_metric_summary(\n",
    "    summary_df: pd.DataFrame, metric: str, per_basin: bool = False, figsize=(10, 6)\n",
    "):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if per_basin:\n",
    "        df_plot = summary_df[metric].unstack(level=0)\n",
    "\n",
    "        # Sort basins based on first horizon values\n",
    "        first_horizon_values = df_plot.iloc[0]\n",
    "        sorted_basins = first_horizon_values.sort_values(ascending=False).index\n",
    "        df_plot = df_plot[sorted_basins]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=df_plot.melt(ignore_index=False).reset_index(),\n",
    "            x=\"horizon\",\n",
    "            y=\"value\",\n",
    "            hue=\"basin_id\",\n",
    "            palette=\"Blues\",\n",
    "        )\n",
    "        plt.title(f\"{metric} by Basin and Horizon\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", title=\"Basin ID\")\n",
    "\n",
    "    else:\n",
    "        ax = sns.barplot(x=summary_df.index, y=summary_df[metric], color=\"steelblue\")\n",
    "        plt.title(f\"Overall {metric} by Horizon\")\n",
    "\n",
    "        for i, v in enumerate(summary_df[metric]):\n",
    "            ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xlabel(\"Forecast Horizon\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "plot_metric_summary(overall_summary, \"NSE\")  # Plot overall NSE\n",
    "plot_metric_summary(\n",
    "    basin_summary, \"NSE\", per_basin=True, figsize=(12, 6)\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
