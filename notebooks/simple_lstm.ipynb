{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids\n",
    "from src.data_models.dataset import HydroDataset\n",
    "from src.data_models.preprocessing import (\n",
    "    scale_time_series,\n",
    "    scale_static_attributes,\n",
    "    inverse_scale_static_attributes,\n",
    ")\n",
    "from src.data_models.datamodule import HydroDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare CAMELS-CH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_config = CamelsCHConfig(\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/timeseries/observation_based/\",\n",
    "    timeseries_pattern=\"CAMELS_CH_obs_based_*.csv\",\n",
    "    static_attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/static_attributes\",\n",
    "    use_climate=False,\n",
    "    use_geology=False,\n",
    "    use_glacier=False,\n",
    "    use_human_influence=False,\n",
    "    use_hydrogeology=False,\n",
    "    use_hydrology=False,\n",
    "    use_landcover=False,\n",
    "    use_soil=False,\n",
    "    use_topographic=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time series data for 10 stations\n"
     ]
    }
   ],
   "source": [
    "all_gauge_ids = get_all_gauge_ids(camels_config)\n",
    "\n",
    "ids_for_training = all_gauge_ids[:10]\n",
    "\n",
    "camels = CamelsCH(camels_config)\n",
    "camels.load_stations(ids_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static = camels.get_static_attributes()\n",
    "# # q_mean\trunoff_ratio\tstream_elas\tslope_fdc\tbaseflow_index_landson\thfd_mean\n",
    "# static = static[[\"gauge_id\", \"q_mean\", \"runoff_ratio\", \"stream_elas\", \"slope_fdc\", \"baseflow_index_landson\", \"hfd_mean\"]]\n",
    "# static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get time series data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = camels.get_time_series()\n",
    "data = data[\n",
    "    [\n",
    "        \"gauge_id\",\n",
    "        \"date\",\n",
    "        \"discharge_spec(mm/d)\",\n",
    "        \"precipitation(mm/d)\",\n",
    "        \"temperature_mean(degC)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# # Split into train/test\n",
    "# df_train, df_test = train_validate_split(data, train_ratio=0.8)\n",
    "\n",
    "# # Scale by basin\n",
    "# scaled_train, scaled_test, params = scale_time_series(\n",
    "#     df_train,\n",
    "#     df_test,\n",
    "#     features=[\"discharge_spec(mm/d)\", \"precipitation(mm/d)\", \"temperature_mean(degC)\"],\n",
    "#     by_basin=True,  # Set False for global scaling\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes = [col for col in static.columns if col != 'gauge_id']\n",
    "\n",
    "# # Scale the attributes\n",
    "# scaled_static_df, scaling_params = scale_static_attributes(static, attributes)\n",
    "\n",
    "# # Add back gauge_id if needed\n",
    "# scaled_static_df['gauge_id'] = static['gauge_id']\n",
    "\n",
    "# # Inverse scale to verify\n",
    "# original_df = inverse_scale_static_attributes(scaled_static_df[attributes], scaling_params)\n",
    "# original_df['gauge_id'] = static['gauge_id']\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nOriginal values:\")\n",
    "# print(static)\n",
    "# print(\"\\nScaled values:\")\n",
    "# print(scaled_static_df)\n",
    "# print(\"\\nInverse scaled values:\")\n",
    "# print(original_df)\n",
    "\n",
    "# # Verify the scaling worked correctly\n",
    "# np.testing.assert_array_almost_equal(\n",
    "#     static[attributes].values,\n",
    "#     original_df[attributes].values\n",
    "# )\n",
    "# print(\"\\nVerification passed: Original and inverse-scaled values match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_config = {\n",
    "    \"features\": {\n",
    "        \"scale_method\": \"per_basin\",\n",
    "        \"log_transform\": []\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"scale_method\": \"per_basin\",\n",
    "        \"log_transform\": False\n",
    "    },\n",
    "    \"static_features\": {\n",
    "        \"scale_method\": \"global\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = HydroDataModule(\n",
    "    time_series_df=data,\n",
    "    static_df=None,\n",
    "    preprocessing_config=preprocessing_config,\n",
    "    batch_size=32,\n",
    "    input_length=30,\n",
    "    output_length=1,\n",
    "    num_workers=4,\n",
    "    features=[\"discharge_spec(mm/d)\", \"precipitation(mm/d)\", \"temperature_mean(degC)\"],\n",
    "    target=\"discharge_spec(mm/d)\",\n",
    "    train_years=20,\n",
    "    val_years=1,\n",
    "    min_test_years=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap the data in a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\"discharge_spec(mm/d)\", \"precipitation(mm/d)\", \"temperature_mean(degC)\"]\n",
    "# target = \"discharge_spec(mm/d)\"\n",
    "\n",
    "# # Create dataset\n",
    "# train_dataset = HydroDataset(\n",
    "#     time_series_df=scaled_train,\n",
    "#     static_df=None,\n",
    "#     input_length=30,\n",
    "#     output_length=5,\n",
    "#     features=features,\n",
    "#     target=target,\n",
    "# )\n",
    "\n",
    "# validate_dataset = HydroDataset(\n",
    "#     time_series_df=scaled_train,\n",
    "#     static_df=None,\n",
    "#     input_length=30,\n",
    "#     output_length=5,\n",
    "#     features=features,\n",
    "#     target=target,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the dataset iterable by creating a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import LitLSTM\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality Check Summary:\n",
      "Original basins: 10\n",
      "Retained basins: 4\n",
      "Excluded basins: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/cooper/Desktop/CAMELS-CH/notebooks/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | SimpleLSTM | 17.7 K | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "17.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.7 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed with temporal splitting:\n",
      "- Using first 20 years of each basin for fitting scalers\n",
      "- Features scaled using per_basin method\n",
      "- Target scaled using per_basin method\n",
      "- Log transforms applied to: [] and target: False\n",
      "Created 29100 valid sequences.\n",
      "Created 1340 valid sequences.\n",
      "\n",
      "Data split summary:\n",
      "Training: 29220 samples from 4 basins\n",
      "Validation: 1460 samples from 4 basins\n",
      "Testing: 27760 samples from 4 basins\n",
      "Epoch 4: 100%|██████████| 910/910 [00:07<00:00, 127.68it/s, v_num=9]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 910/910 [00:07<00:00, 127.66it/s, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "model = LitLSTM(\n",
    "    input_size=3,\n",
    "    hidden_size=64,\n",
    "    num_layers=1,\n",
    "    output_size=1,\n",
    "    target=\"discharge_spec(mm/d)\",\n",
    ")\n",
    "\n",
    "\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evalue and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = data_module.quality_report\n",
    "\n",
    "excluded_basins = list(quality_report['excluded_basins'].keys())\n",
    "excluded_basins\n",
    "\n",
    "ids_for_training = [id for id in ids_for_training if id not in excluded_basins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality Check Summary:\n",
      "Original basins: 10\n",
      "Retained basins: 4\n",
      "Excluded basins: 6\n",
      "Data preprocessing completed with temporal splitting:\n",
      "- Using first 20 years of each basin for fitting scalers\n",
      "- Features scaled using per_basin method\n",
      "- Target scaled using per_basin method\n",
      "- Log transforms applied to: [] and target: False\n",
      "Created 27640 valid sequences.\n",
      "\n",
      "Data split summary:\n",
      "Training: 29220 samples from 4 basins\n",
      "Validation: 1460 samples from 4 basins\n",
      "Testing: 27760 samples from 4 basins\n",
      "Testing DataLoader 0: 100%|██████████| 864/864 [00:02<00:00, 343.06it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss          0.029699645936489105\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model, data_module)\n",
    "test_results = model.test_results\n",
    "\n",
    "# Get predictions and transform\n",
    "df_pred = pd.DataFrame(test_results[\"predictions\"], columns=[model.hparams.target])\n",
    "df_pred[\"gauge_id\"] = test_results[\"basin_ids\"]\n",
    "df_targets = pd.DataFrame(test_results[\"targets\"], columns=[model.hparams.target])\n",
    "df_targets[\"gauge_id\"] = test_results[\"basin_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11790432, -0.45984522, -0.5488223 , ...,  0.15254001,\n",
       "       -0.04198272, -0.19649418], shape=(27640,), dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inversed_preds = data_module.inverse_transform_predictions(\n",
    "    test_results[\"predictions\"], basin_ids=test_results[\"basin_ids\"]\n",
    ").to_numpy()\n",
    "inversed_targets = data_module.inverse_transform_predictions(\n",
    "    test_results[\"targets\"], basin_ids=test_results[\"basin_ids\"]\n",
    ").to_numpy()\n",
    "\n",
    "inversed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot for each basin\n",
    "for specific_basin_id in ids_for_training:\n",
    "    basin_indices = np.where(test_results[\"basin_ids\"] == specific_basin_id)[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        x=range(min(365 * 3, len(inversed_targets[basin_indices]))),\n",
    "        y=inversed_targets[basin_indices][: 365 * 3],\n",
    "        label=\"Actual\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=range(min(365 * 3, len(inversed_preds[basin_indices]))),\n",
    "        y=inversed_preds[basin_indices][: 365 * 3],\n",
    "        label=\"Predicted\",\n",
    "        color=\"red\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Timestep\", fontsize=12)\n",
    "    plt.ylabel(\"Discharge\", fontsize=12)\n",
    "    plt.title(\n",
    "        f\"Predicted vs Actual Discharge for Basin {specific_basin_id}\", fontsize=14\n",
    "    )\n",
    "    plt.legend(fontsize=12, loc=\"upper left\")\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
