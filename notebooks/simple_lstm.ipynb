{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids\n",
    "from src.data_models.dataset import HydroDataset\n",
    "from src.data_models.preprocessing import (\n",
    "    scale_time_series,\n",
    "    scale_static_attributes,\n",
    "    inverse_scale_static_attributes,\n",
    "    inverse_scale_time_series,\n",
    ")\n",
    "from src.data_models.caravanify import Caravanify, CaravanifyConfig\n",
    "\n",
    "from utils.metrics import nash_sutcliffe_efficiency\n",
    "from src.data_models.datamodule import HydroDataModule\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from src.preprocessing.grouped import GroupedTransformer\n",
    "from src.preprocessing.log_scale import LogTransformer\n",
    "from src.preprocessing.standard_scale import StandardScaleTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Caravanify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CH/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CH/post_processed/timeseries/csv\",\n",
    "    gauge_id_prefix=\"CH\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "\n",
    "caravan = Caravanify(config)\n",
    "# ids_for_training = [\n",
    "#     \"CA_15016\",\n",
    "#     \"CA_17462\",\n",
    "# ]\n",
    "ids_for_training = caravan.get_all_gauge_ids()[3:5]\n",
    "\n",
    "print(f\"Total number of stations: {len(ids_for_training)}\")\n",
    "\n",
    "caravan.load_stations(ids_for_training)\n",
    "\n",
    "\n",
    "# Get data\n",
    "ts_data = caravan.get_time_series()\n",
    "static_data = caravan.get_static_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data[\"date\"] = pd.to_datetime(ts_data[\"date\"])\n",
    "\n",
    "# Now we can get the day of year using dt accessor\n",
    "ts_data[\"julian_day\"] = ts_data[\"date\"].dt.dayofyear\n",
    "\n",
    "# Get column names excluding specific columns\n",
    "ts_columns = [\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"streamflow\",\n",
    "    # \"julian_day\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"total_precipitation_sum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = ts_columns + [\"gauge_id\", \"date\"]\n",
    "ts_data = ts_data[whole_data]\n",
    "\n",
    "# # Group by gauge_id\n",
    "# grouped = ts_data.groupby(\"gauge_id\")\n",
    "\n",
    "# # Plot time series for each gauge of the ts_columns of the last 5 years\n",
    "# for gauge_id, group in grouped:\n",
    "#     group = group.set_index(\"date\")\n",
    "#     group = group.loc[\"2015-01-01\":\"2020-12-31\"]\n",
    "#     group[ts_columns].plot(subplots=True, figsize=(20, 20), title=gauge_id)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statics_to_keep = [\n",
    "    \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "static_columns = static_data.columns\n",
    "static_columns = [col for col in list(static_columns) if col in statics_to_keep]\n",
    "\n",
    "static_data = static_data[static_columns]\n",
    "static_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    col for col in ts_data.columns if col not in [\"gauge_id\", \"date\", \"streamflow\"]\n",
    "]\n",
    "ts_columns = features + [\"streamflow\"]  # Ensure target is not in features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare CAMELS-CH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camels_config = CamelsCHConfig(\n",
    "#     timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/timeseries/observation_based/\",\n",
    "#     timeseries_pattern=\"CAMELS_CH_obs_based_*.csv\",\n",
    "#     static_attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/static_attributes\",\n",
    "#     use_climate=False,\n",
    "#     use_geology=False,\n",
    "#     use_glacier=False,\n",
    "#     use_human_influence=False,\n",
    "#     use_hydrogeology=False,\n",
    "#     use_hydrology=False,\n",
    "#     use_landcover=False,\n",
    "#     use_soil=False,\n",
    "#     use_topographic=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_gauge_ids = get_all_gauge_ids(camels_config)\n",
    "\n",
    "# ids_for_training = all_gauge_ids[:5]\n",
    "\n",
    "# camels = CamelsCH(camels_config)\n",
    "# camels.load_stations(ids_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = camels.get_time_series()\n",
    "# data = data[\n",
    "#     [\n",
    "#         \"gauge_id\",\n",
    "#         \"date\",\n",
    "#         \"discharge_spec(mm/d)\",\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static = camels.get_static_attributes()\n",
    "# sc = static.columns\n",
    "\n",
    "# # for i in range(len(sc)):\n",
    "# #     print(f\"{i}: {sc[i]}\")\n",
    "# static_attributes = [\n",
    "#     \"gauge_id\",\n",
    "#     \"area\",\n",
    "#     \"elev_mean\",\n",
    "#     \"slope_mean\",\n",
    "#     \"aridity\",\n",
    "#     \"p_seasonality\",\n",
    "#     \"frac_snow\",\n",
    "#     \"porosity\",\n",
    "#     \"conductivity\",\n",
    "#     \"p_mean\",\n",
    "#     \"geo_porosity\",\n",
    "# ]\n",
    "# static = static[static_attributes]\n",
    "# static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_feature_cols = features\n",
    "static_feature_cols = [c for c in static_columns if c != \"gauge_id\"]\n",
    "target_cols = [\"streamflow\"]\n",
    "\n",
    "# Feature pipeline: log + scale\n",
    "feature_pipeline = Pipeline(\n",
    "    [\n",
    "        # (\"log\", LogTransformer(columns=dynamic_feature_cols)),\n",
    "        (\"scaler\", StandardScaleTransformer(columns=dynamic_feature_cols))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Target pipeline: grouped by basin with log + scale\n",
    "target_pipeline = GroupedTransformer(\n",
    "    Pipeline(\n",
    "        [\n",
    "            # (\"log\", LogTransformer(columns=target_cols)),\n",
    "            (\"scaler\", StandardScaleTransformer(columns=target_cols))\n",
    "        ]\n",
    "    ),\n",
    "    columns=target_cols,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Static feature pipeline: just scaling\n",
    "static_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaleTransformer(columns=static_feature_cols))]\n",
    ")\n",
    "\n",
    "# Define preprocessing configurations\n",
    "preprocessing_configs = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline, \"columns\": dynamic_feature_cols},\n",
    "    \"target\": {\"pipeline\": target_pipeline, \"columns\": target_cols},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_feature_cols},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 10\n",
    "input_length = 40\n",
    "\n",
    "static_columns = [c for c in static_columns if c not in [\"gauge_id\"]]\n",
    "\n",
    "print(\"TS columns:\", ts_columns)\n",
    "print(\"Static columns:\", static_columns)\n",
    "\n",
    "\n",
    "data_module = HydroDataModule(\n",
    "    time_series_df=ts_data,\n",
    "    static_df=static_data,\n",
    "    # static_df=None,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    preprocessing_config=preprocessing_configs,\n",
    "    batch_size=128,\n",
    "    input_length=input_length,\n",
    "    output_length=output_length,\n",
    "    num_workers=4,\n",
    "    features=ts_columns,\n",
    "    static_features=static_columns,\n",
    "    # static_features=None,\n",
    "    target=\"streamflow\",\n",
    "    min_train_years=2,\n",
    "    val_years=1,\n",
    "    test_years=1,\n",
    "    max_missing_pct=10,\n",
    "    domain_id=\"CA\",\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "# train_loader = data_module.test_dataloader()\n",
    "\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     if i >= 6:  # Look at first 6 batches\n",
    "#         break\n",
    "\n",
    "#     print(f\"Slice indeces: {batch['slice_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import LitLSTM\n",
    "from src.models.ealstm import LitEALSTM\n",
    "from src.models.TSMixer import LitTSMixer, TSMixerConfig\n",
    "from src.models.evaluators import TSForecastEvaluator\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evalue and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LitLSTM(\n",
    "#     input_size=len(ts_columns),\n",
    "#     hidden_size=16,\n",
    "#     num_layers=1,\n",
    "#     output_size=output_length,\n",
    "#     target=data_module.target,\n",
    "# )\n",
    "\n",
    "# model = LitEALSTM(\n",
    "#     input_size_dyn=len(ts_columns),\n",
    "#     input_size_stat=len(static_columns) - 1,\n",
    "#     hidden_size=64,\n",
    "#     output_size=output_length,\n",
    "# )\n",
    "\n",
    "config = TSMixerConfig(\n",
    "    input_len=input_length,\n",
    "    output_len=output_length,\n",
    "    input_size=len(ts_columns),\n",
    "    static_size=len(static_columns),\n",
    "    hidden_size=80,\n",
    "    learning_rate=7e-4,\n",
    "    dropout=0.1,\n",
    "    num_layers=2,\n",
    ")\n",
    "\n",
    "model = LitTSMixer(config)\n",
    "\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = data_module.quality_report\n",
    "\n",
    "excluded_basins = list(quality_report[\"excluded_basins\"].keys())\n",
    "excluded_basins\n",
    "\n",
    "ids_for_training = [id for id in ids_for_training if id not in excluded_basins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, data_module)\n",
    "raw_results = model.test_results\n",
    "\n",
    "# Create evaluator and get metrics\n",
    "evaluator = TSForecastEvaluator(\n",
    "    data_module, horizons=list(range(1, model.config.output_len + 1))\n",
    ")\n",
    "results_df, overall_metrics, basin_metrics = evaluator.evaluate(raw_results)\n",
    "\n",
    "# Get overall summary\n",
    "overall_summary = evaluator.summarize_metrics(overall_metrics)\n",
    "\n",
    "# Get per-basin summary\n",
    "basin_summary = evaluator.summarize_metrics(basin_metrics, per_basin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.test_results = raw_results\n",
    "\n",
    "# Assuming you have an evaluator with test results already populated\n",
    "fig, ax = evaluator.plot_rolling_forecast(\n",
    "    horizon=1,\n",
    "    group_identifier=\"CH_2019\",\n",
    "    datamodule=data_module,\n",
    "    y_label=\"Streamflow (m³/s)\",\n",
    "    debug=True,\n",
    "    line_style_forecast=\"-\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_metric_summary(\n",
    "    summary_df: pd.DataFrame, metric: str, per_basin: bool = False, figsize=(10, 6)\n",
    "):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if per_basin:\n",
    "        df_plot = summary_df[metric].unstack(level=0)\n",
    "\n",
    "        # Sort basins based on first horizon values\n",
    "        first_horizon_values = df_plot.iloc[0]\n",
    "        sorted_basins = first_horizon_values.sort_values(ascending=False).index\n",
    "        df_plot = df_plot[sorted_basins]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=df_plot.melt(ignore_index=False).reset_index(),\n",
    "            x=\"horizon\",\n",
    "            y=\"value\",\n",
    "            hue=\"basin_id\",\n",
    "            palette=\"Blues\",\n",
    "        )\n",
    "        plt.title(f\"{metric} by Basin and Horizon\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", title=\"Basin ID\")\n",
    "\n",
    "    else:\n",
    "        ax = sns.barplot(x=summary_df.index, y=summary_df[metric], color=\"steelblue\")\n",
    "        plt.title(f\"Overall {metric} by Horizon\")\n",
    "\n",
    "        for i, v in enumerate(summary_df[metric]):\n",
    "            ax.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.xlabel(\"Forecast Horizon\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "plot_metric_summary(overall_summary, \"NSE\")  # Plot overall NSE\n",
    "plot_metric_summary(basin_summary, \"NSE\", per_basin=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
