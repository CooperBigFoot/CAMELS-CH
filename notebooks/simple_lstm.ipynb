{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/pytorch_forecasting/models/base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import glob \n",
    "from pathlib import Path\n",
    "\n",
    "from src.benchmark_tft.data_loading import combine_camels_data\n",
    "from src.data_models.camels_ch import CamelsCH, CamelsCHConfig, get_all_gauge_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time series data for 1 stations\n"
     ]
    }
   ],
   "source": [
    "camels_config = CamelsCHConfig(\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/timeseries/observation_based/\",\n",
    "    timeseries_pattern=\"CAMELS_CH_obs_based_*.csv\",\n",
    "    static_attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/static_attributes\",\n",
    "    use_climate=False,\n",
    "    use_geology=False,\n",
    "    use_glacier=False,\n",
    "    use_human_influence=False,\n",
    "    use_hydrogeology=False,\n",
    "    use_hydrology=False,\n",
    "    use_landcover=False,\n",
    "    use_soil=False,\n",
    "    use_topographic=False,\n",
    ")\n",
    "\n",
    "# gauge_ids = get_all_gauge_ids(camels_config)\n",
    "\n",
    "# print(f\"There are {len(gauge_ids)} gauge ids\")\n",
    "\n",
    "camels = CamelsCH(camels_config)\n",
    "camels.load_stations([\"2486\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>discharge_spec(mm/d)</th>\n",
       "      <th>precipitation(mm/d)</th>\n",
       "      <th>temperature_mean(degC)</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.91</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.48</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.89</td>\n",
       "      <td>-5.07</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14605</th>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14606</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>2.048</td>\n",
       "      <td>6.02</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>1.725</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14608</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>1.587</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14609</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.426</td>\n",
       "      <td>7.91</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14610 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  discharge_spec(mm/d)  precipitation(mm/d)  \\\n",
       "0     1981-01-01                   NaN                 2.47   \n",
       "1     1981-01-02                   NaN                 1.46   \n",
       "2     1981-01-03                   NaN                28.91   \n",
       "3     1981-01-04                   NaN                23.48   \n",
       "4     1981-01-05                   NaN                 7.89   \n",
       "...          ...                   ...                  ...   \n",
       "14605 2020-12-27                 2.240                 2.66   \n",
       "14606 2020-12-28                 2.048                 6.02   \n",
       "14607 2020-12-29                 1.725                 3.41   \n",
       "14608 2020-12-30                 1.587                 2.92   \n",
       "14609 2020-12-31                 1.426                 7.91   \n",
       "\n",
       "       temperature_mean(degC) gauge_id  \n",
       "0                       -0.97     2486  \n",
       "1                       -2.45     2486  \n",
       "2                        1.15     2486  \n",
       "3                        0.84     2486  \n",
       "4                       -5.07     2486  \n",
       "...                       ...      ...  \n",
       "14605                   -2.03     2486  \n",
       "14606                   -0.45     2486  \n",
       "14607                   -0.04     2486  \n",
       "14608                   -1.99     2486  \n",
       "14609                   -1.24     2486  \n",
       "\n",
       "[14610 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = camels.get_time_series()\n",
    "data = data[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"discharge_spec(mm/d)\",\n",
    "        \"precipitation(mm/d)\",\n",
    "        \"temperature_mean(degC)\",\n",
    "        \"gauge_id\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"discharge_spec(mm/d)\"])\n",
    "\n",
    "data.loc[:, \"precipitation(mm/d)\"] = data[\"precipitation(mm/d)\"].fillna(0)\n",
    "\n",
    "data.loc[:, \"temperature_mean(degC)\"] = data[\"temperature_mean(degC)\"].fillna(\n",
    "    data[\"temperature_mean(degC)\"].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/kqwqph4s3sj7hkxryrly5y6r0000gn/T/ipykernel_46810/1799506231.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, \"time_idx\"] = data[\"date\"].rank(method=\"dense\").astype(int) - 1\n"
     ]
    }
   ],
   "source": [
    "data.loc[:, \"time_idx\"] = data[\"date\"].rank(method=\"dense\").astype(int) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 365\n",
    "max_prediction_length = 1\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length * 365 \n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "   data[lambda x: x[\"time_idx\"] <= training_cutoff],\n",
    "   time_idx=\"time_idx\",\n",
    "   target=\"discharge_spec(mm/d)\", \n",
    "   group_ids=[\"gauge_id\"],\n",
    "   max_encoder_length=max_encoder_length,\n",
    "   min_encoder_length=max_encoder_length // 2,\n",
    "   max_prediction_length=max_prediction_length,\n",
    "   min_prediction_length=1,\n",
    "   time_varying_known_reals=[\"precipitation(mm/d)\", \"temperature_mean(degC)\"],\n",
    "   time_varying_unknown_reals=[\"discharge_spec(mm/d)\"],\n",
    "   target_normalizer=GroupNormalizer(groups=[\"gauge_id\"]),\n",
    "   add_relative_time_idx=True,\n",
    "   add_target_scales=True,\n",
    "   add_encoder_length=True,\n",
    "   allow_missing_timesteps=True\n",
    ")\n",
    "\n",
    "# Create validation set using last max_prediction_length timesteps\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "   training,\n",
    "   data[lambda x: x[\"time_idx\"] > training_cutoff],\n",
    "   predict=True,\n",
    "   stop_randomization=True\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\"),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=[0] if torch.cuda.is_available() else 1,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,\n",
    "    enable_checkpointing=True,\n",
    "    logger=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/cooper/Desktop/CAMELS-CH/notebooks/checkpoints exists and is not empty.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 112    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.7 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 2.4 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K  | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "20.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.1 K    Total params\n",
      "0.080     Total estimated model params size (MB)\n",
      "302       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 50/50 [00:18<00:00,  2.74it/s, v_num=22, train_loss_step=0.308, val_loss=0.0838, train_loss_epoch=0.401]\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer=\"adam\",\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/pytorch_forecasting/metrics/base_metrics.py:817: UserWarning: Loss is not finite. Resetting it to 1e9\n",
      "  warnings.warn(\"Loss is not finite. Resetting it to 1e9\")\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cooper/Desktop/CAMELS-CH/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE: 1000000000.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m hindcast_predictions \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(hindcast_dataloader)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# You can then compare hindcast_predictions with the known historical targets\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m best_model\u001b[38;5;241m.\u001b[39mplot_prediction(\u001b[43mhindcast_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m, hindcast_predictions\u001b[38;5;241m.\u001b[39moutput, idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "best_model_path = \"/Users/cooper/Desktop/CAMELS-CH/notebooks/checkpoints/tft-epoch=00-val_loss=0.16.ckpt\"\n",
    "\n",
    "hindcast_cutoff = data[\"time_idx\"].max() - max_prediction_length * 365\n",
    "\n",
    "# Load the best model and set to eval mode\n",
    "best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "# Evaluate on a validation/test dataset (built via from_dataset)\n",
    "predictions = best_model.predict(val_dataloader, return_y=True)\n",
    "# Compute a metric (e.g., SMAPE)\n",
    "smape = SMAPE()(predictions.output, predictions.y)\n",
    "print(\"SMAPE:\", smape.item())\n",
    "\n",
    "# Hindcast example: create a hindcast dataset (using predict_mode=True ensures only the last forecast point is used)\n",
    "hindcast_dataset = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    data[lambda x: x[\"time_idx\"] > hindcast_cutoff],\n",
    "    predict=True,\n",
    "    stop_randomization=True\n",
    ")\n",
    "hindcast_dataloader = hindcast_dataset.to_dataloader(train=False, batch_size=128)\n",
    "hindcast_predictions = best_model.predict(hindcast_dataloader)\n",
    "# You can then compare hindcast_predictions with the known historical targets\n",
    "best_model.plot_prediction(hindcast_predictions.x, hindcast_predictions.output, idx=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
